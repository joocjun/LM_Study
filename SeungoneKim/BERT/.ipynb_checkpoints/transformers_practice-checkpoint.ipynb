{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using transformers\n",
    "<br>\n",
    "Using the transformers library which implemented various NLP Attention based models, we could do a lot of tasks quite easily.<br>\n",
    "In this notebook, I will do various NLP tasks using Bert model and GPT model.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "<br>\n",
    "Pipelines are a great and easy way to use models for inference.<br>\n",
    "Pipelines are objects that abstract most of the complex code from the library, offering a simple API.<br>\n",
    "<br>\n",
    "When choosing a model for a certain task, we should carefully decide.<br>\n",
    "<br>\n",
    "For example, BERT is primarily aimed at being fine-tuned on tasks that use the whole sentence to make decisions.<br>\n",
    "This includes, sequence classification, token classification, or question answering.<br>\n",
    "Therefore, tasks such as text generation could be done using AR models such as GPT instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cb2468e2ce41ef8e7652012cc7ae31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction pipeline extracts hidden states from the base transformers, \n",
    "# which can be used as features in downstream tasks.\n",
    "feature_extraction_bert = pipeline('feature-extraction',model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: LABEL_0, with score: 0.7663\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis is classifying sequences according to positive or negative sentiments.\n",
    "# In the general TextClassificationPipeline, if multiple classification labels are available, \n",
    "# the pipeline will run a softmax over results.\n",
    "\n",
    "# The most famous dataset to finetune on sentiment analysis is \n",
    "# 'SST-2 dataset(Stanford Sentiment Treebank)' from the \n",
    "# 'GLUE dataset(General Language Understanding Evaluation)'\n",
    "\n",
    "# A label('POSITIVE' or 'NEGATIVE') is returned alongside a score\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "tokenizer= BertTokenizer.from_pretrained('bert-base-cased-finetuned-mrpc')\n",
    "model= BertForSequenceClassification.from_pretrained('bert-base-cased-finetuned-mrpc')\n",
    "\n",
    "# Actually, the default model is DistilBERT model that was fine-tuned on SST2 from GLUE dataset.\n",
    "sentiment_analysis_bert = pipeline('sentiment-analysis',tokenizer=tokenizer,model=model)\n",
    "\n",
    "result = sentiment_analysis_bert(\"I love you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'],4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entity_recognition_bert = pipeline('ner',model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'Perhaps this is god', score: 0.2784, start: 22, end: 41\n"
     ]
    }
   ],
   "source": [
    "# Question Answering is extracting an answer from a text given a question.\n",
    "\n",
    "# The most famous dataset is the 'SQuAD dataset(Stanford Question Answering Dataset)'\n",
    "\n",
    "# One thing to mind is that only Extractive Question Answering is available.\n",
    "# This means that a context should be given\n",
    "# A answer extracted from the given text and a confidence score is returned.\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "context = \"Who is this speaking? Perhaps this is god speaking. Then it is likely he will exist\"\n",
    "\n",
    "tokenizer= BertTokenizer.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')\n",
    "model= BertForQuestionAnswering.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "question_answering_bert = pipeline('question-answering',model=model,tokenizer=tokenizer)\n",
    "\n",
    "result = question_answering_bert(question='Do you think there exists god?', context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'],4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': 'the best university in south korea is seoul university.', 'score': 0.4197063446044922, 'token': 10884, 'token_str': 'seoul'}, {'sequence': 'the best university in south korea is korea university.', 'score': 0.17043077945709229, 'token': 4420, 'token_str': 'korea'}, {'sequence': 'the best university in south korea is samsung university.', 'score': 0.060363661497831345, 'token': 19102, 'token_str': 'samsung'}, {'sequence': 'the best university in south korea is peking university.', 'score': 0.013846565037965775, 'token': 27057, 'token_str': 'peking'}, {'sequence': 'the best university in south korea is sbs university.', 'score': 0.01384267583489418, 'token': 21342, 'token_str': 'sbs'}]\n"
     ]
    }
   ],
   "source": [
    "# Language Modeling is a task of fitting a model to a corpus.\n",
    "# For example, BERT uses masked language modeling, and GPT uses casual language modeling.\n",
    "# Language Modeling can be useful outside of pretraining as well as domain specific.\n",
    "# Especially, Masked Language Modeling is the task of filling the mask with an appropriate token.\n",
    "\n",
    "fill_mask_bert = pipeline('fill-mask',model='bert-base-uncased')\n",
    "\n",
    "result = fill_mask_bert(\"The best university in South Korea is [MASK] university.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_bert = pipeline('summarization',model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_bert = pipeline('translation_en_to_kr',model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2text_bert = pipeline('text2text-generation',model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_classification_bert = pipeline('zero-shot-classification',model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_bert = pipeline('conversational',model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(3, 13), dtype=int32, numpy=\n",
      "array([[  101,  2003,  2023,  1037,  9398,  6251,  1029,   102,     0,\n",
      "            0,     0,     0,     0],\n",
      "       [  101,  1045,  2572,  3076,  2770,  2000, 10930, 12325,  2072,\n",
      "         2118,  2311,  1012,   102],\n",
      "       [  101,  2026,  5440,  4730,  2653,  2003,  1039,  1009,  1009,\n",
      "         1012,   102,     0,     0]])>, 'token_type_ids': <tf.Tensor: shape=(3, 13), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(3, 13), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])>}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model= TFBertModel.from_pretrained('bert-base-uncased')\n",
    "# tokenizer -> dataloader(preprocessing data) -> model\n",
    "text1 =\"Is this a valid sentence?\"\n",
    "text2 = \"I am student running to Yonsei University building.\"\n",
    "text3 = \" My favorite Programming Language is C++.\"\n",
    "text = [text1,text2,text3]\n",
    "encoded_inputs = tokenizer(text, return_tensors='tf',return_attention_mask=True,padding=True,truncation=True)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 13), dtype=int32, numpy=\n",
       "array([[  101,  2003,  2023,  1037,  9398,  6251,  1029,   102,     0,\n",
       "            0,     0,     0,     0],\n",
       "       [  101,  1045,  2572,  3076,  2770,  2000, 10930, 12325,  2072,\n",
       "         2118,  2311,  1012,   102],\n",
       "       [  101,  2026,  5440,  4730,  2653,  2003,  1039,  1009,  1009,\n",
       "         1012,   102,     0,     0]])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] is this a valid sentence? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] i am student running to yonsei university building. [SEP]\n",
      "[CLS] my favorite programming language is c + +. [SEP] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(encoded_inputs['input_ids'][0]))\n",
    "print(tokenizer.decode(encoded_inputs['input_ids'][1]))\n",
    "print(tokenizer.decode(encoded_inputs['input_ids'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 101, 2054, 2003, 2115, 2171, 1029,  102, 2026, 2171, 2003, 7367,\n",
      "        5575, 5643, 1012,  102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]])>, 'attention_mask': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}\n"
     ]
    }
   ],
   "source": [
    "encoded_input2 = tokenizer(\"What is your name?\",\"My name is Seungone.\",return_tensors='tf')\n",
    "print(encoded_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] what is your name? [SEP] my name is seungone. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(encoded_input2['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor: shape=(3, 13, 768), dtype=float32, numpy=\n",
      "array([[[ 0.1584683 ,  0.05394202, -0.02173148, ..., -0.1263333 ,\n",
      "          0.06896812,  0.46639672],\n",
      "        [ 0.12491301, -0.1220137 , -0.03282016, ...,  0.3831069 ,\n",
      "          1.0131848 ,  0.6159238 ],\n",
      "        [-0.20566858, -0.6244838 ,  0.16080046, ..., -0.00229507,\n",
      "         -0.05117407,  0.5581342 ],\n",
      "        ...,\n",
      "        [ 0.22871093,  0.44744945,  0.3578546 , ...,  0.46337926,\n",
      "          0.10052502, -0.2444902 ],\n",
      "        [ 0.18982568, -0.05153685, -0.08534762, ...,  0.3760352 ,\n",
      "          0.197602  , -0.27288702],\n",
      "        [ 0.1412916 , -0.01042851, -0.01311123, ...,  0.36208832,\n",
      "          0.18744272, -0.21668495]],\n",
      "\n",
      "       [[-0.22801131,  0.15609686,  0.22225544, ..., -0.577901  ,\n",
      "          0.35622063,  0.18715926],\n",
      "        [-0.16532849, -0.3931642 ,  0.09108079, ..., -0.5639909 ,\n",
      "          0.07929863, -0.29892743],\n",
      "        [-0.3986258 , -0.07625765,  0.6550277 , ..., -0.6394473 ,\n",
      "         -0.02087855, -0.04348911],\n",
      "        ...,\n",
      "        [ 0.7989109 , -0.00488814,  0.1144812 , ..., -0.8205821 ,\n",
      "         -0.1688754 , -0.27963817],\n",
      "        [-0.52072173, -0.5619536 , -0.10707913, ..., -0.20739725,\n",
      "          0.1605132 , -0.80419934],\n",
      "        [ 0.6672145 ,  0.01286071, -0.06178375, ..., -0.01073188,\n",
      "         -0.38118887, -0.16385065]],\n",
      "\n",
      "       [[-0.05409688,  0.06028312, -0.31289005, ..., -0.19278517,\n",
      "          0.17588183,  0.8003695 ],\n",
      "        [ 0.20993368, -0.25410283, -0.2210229 , ..., -0.05778479,\n",
      "          0.03122615,  0.5200929 ],\n",
      "        [ 0.09455551,  0.22572473,  0.1967863 , ..., -0.10307858,\n",
      "          0.07740697,  0.42560345],\n",
      "        ...,\n",
      "        [ 0.428047  ,  0.02009438,  0.1980676 , ...,  0.38901466,\n",
      "         -0.6555018 , -0.18332328],\n",
      "        [ 0.03925106, -0.0317933 ,  0.15678586, ...,  0.29969496,\n",
      "          0.02135993,  0.54023015],\n",
      "        [-0.13839093, -0.3477006 ,  0.06529973, ...,  0.48509985,\n",
      "          0.01171008,  0.4221092 ]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(3, 768), dtype=float32, numpy=\n",
      "array([[-0.9054412 , -0.24784759, -0.46824443, ..., -0.4176028 ,\n",
      "        -0.6105537 ,  0.9119092 ],\n",
      "       [-0.9127721 , -0.62599826, -0.9887003 , ..., -0.97545004,\n",
      "        -0.7618873 ,  0.93983114],\n",
      "       [-0.8769176 , -0.48585215, -0.9049975 , ..., -0.78686345,\n",
      "        -0.6910988 ,  0.8807093 ]], dtype=float32)>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "output = model(encoded_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFMaskedLMOutput(loss=<tf.Tensor: shape=(9,), dtype=float32, numpy=\n",
      "array([1.2178121e+01, 5.3454580e+00, 9.5798366e-04, 6.0269046e-03,\n",
      "       3.3788753e-03, 5.6930067e-04, 8.1031382e-01, 4.1392818e-04,\n",
      "       2.3580479e+01], dtype=float32)>, logits=<tf.Tensor: shape=(1, 9, 28996), dtype=float32, numpy=\n",
      "array([[[ -7.1545234,  -6.9931316,  -7.182646 , ...,  -5.9123926,\n",
      "          -5.6732936,  -5.9854193],\n",
      "        [ -8.018968 ,  -8.13193  ,  -8.050915 , ...,  -6.5679274,\n",
      "          -6.4058185,  -6.8997593],\n",
      "        [ -4.977202 ,  -6.1780906,  -6.066909 , ...,  -5.636203 ,\n",
      "          -4.6602654,  -5.124104 ],\n",
      "        ...,\n",
      "        [ -3.4420216,  -3.2557287,  -3.5732915, ...,  -2.460596 ,\n",
      "          -2.6494863,  -3.195194 ],\n",
      "        [-10.588984 , -10.462034 , -11.718096 , ...,  -7.4646177,\n",
      "          -9.954243 ,  -8.39268  ],\n",
      "        [-14.889968 , -14.887321 , -14.45686  , ..., -11.658825 ,\n",
      "         -13.0151005, -11.607314 ]]], dtype=float32)>, hidden_states=None, attentions=None)\n",
      "\n",
      "tf.Tensor(\n",
      "[1.2178121e+01 5.3454580e+00 9.5798366e-04 6.0269046e-03 3.3788753e-03\n",
      " 5.6930067e-04 8.1031382e-01 4.1392818e-04 2.3580479e+01], shape=(9,), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[[ -7.1545234  -6.9931316  -7.182646  ...  -5.9123926  -5.6732936\n",
      "    -5.9854193]\n",
      "  [ -8.018968   -8.13193    -8.050915  ...  -6.5679274  -6.4058185\n",
      "    -6.8997593]\n",
      "  [ -4.977202   -6.1780906  -6.066909  ...  -5.636203   -4.6602654\n",
      "    -5.124104 ]\n",
      "  ...\n",
      "  [ -3.4420216  -3.2557287  -3.5732915 ...  -2.460596   -2.6494863\n",
      "    -3.195194 ]\n",
      "  [-10.588984  -10.462034  -11.718096  ...  -7.4646177  -9.954243\n",
      "    -8.39268  ]\n",
      "  [-14.889968  -14.887321  -14.45686   ... -11.658825  -13.0151005\n",
      "   -11.607314 ]]], shape=(1, 9, 28996), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForMaskedLM\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertForMaskedLM.from_pretrained('bert-base-cased')\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"tf\")\n",
    "inputs[\"labels\"] = tokenizer(\"The capital of France is Paris.\", return_tensors=\"tf\")[\"input_ids\"]\n",
    "\n",
    "outputs = model(inputs)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "print(outputs)\n",
    "print()\n",
    "print(loss)\n",
    "print()\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': 'The capital of France is Paris.', 'score': 0.44471848011016846, 'token': 2123, 'token_str': 'P a r i s'}, {'sequence': 'The capital of France is Lyon.', 'score': 0.09396772086620331, 'token': 10067, 'token_str': 'L y o n'}, {'sequence': 'The capital of France is Toulouse.', 'score': 0.0823521688580513, 'token': 18367, 'token_str': 'T o u l o u s e'}, {'sequence': 'The capital of France is Lille.', 'score': 0.07515732944011688, 'token': 25411, 'token_str': 'L i l l e'}, {'sequence': 'The capital of France is Marseille.', 'score': 0.05692743510007858, 'token': 17851, 'token_str': 'M a r s e i l l e'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import FillMaskPipeline\n",
    "\n",
    "pipeline = FillMaskPipeline(model,tokenizer)\n",
    "\n",
    "result = pipeline(\"The capital of France is [MASK].\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/transformers/main_classes/pipelines.html#transformers.QuestionAnsweringPipeline\n",
    "https://huggingface.co/transformers/task_summary.html\n",
    "https://huggingface.co/deepset/bert-base-cased-squad2?text=What+is+my+name%3F\n",
    "https://huggingface.co/transformers/model_doc/bert.html\n",
    "https://huggingface.co/tuner007/t5_abs_qa\n",
    "https://huggingface.co/transformers/pretrained_models.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
